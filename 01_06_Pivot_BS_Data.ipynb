{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_06_Pivot_BS_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will transform the verticalized data rows of the BalanceSheet into a horizontalized dataframe.\n",
    "<br>\n",
    "Currently, our data looks similar to the table below. Every Value is placed on its own row.\n",
    "\n",
    "\n",
    "| bs_id | company    | date       | attribute | value |\n",
    "|-------|------------|------------|-----------|-------|\n",
    "| 1     | VitaSport  | 31.10.2018 | Assets    | 100   |\n",
    "| 1     | VitaSport  | 31.10.2018 | Cash      | 80    |\n",
    "| 1     | VitaSport  | 31.10.2018 | Other     | 20    |\n",
    "| 2     | VitaSport  | 31.10.2019 | Assets    | 120   |\n",
    "| 2     | VitaSport  | 31.10.2019 | Cash      | 80    |\n",
    "| 2     | VitaSport  | 31.10.2019 | Other     | 40    |\n",
    "| 3     | GloryFood  | 31.10.2019 | Assets    | 50    |\n",
    "| 3     | GloryFood  | 31.10.2019 | Cash      | 5     |\n",
    "| 3     | GloryFood  | 31.10.2019 | Other     | 45    |\n",
    "\n",
    "<br>\n",
    "But what we would like to have one entry per BalanceSheet:\n",
    "\n",
    "| bs_id | company   | date       | Assets | Cash | Other |\n",
    "|-------|-----------|------------|--------|------|-------|\n",
    "| 1     | VitaSport | 31.10.2018 | 100    | 80   | 20    |\n",
    "| 2     | VitaSport | 31.10.2019 | 120    | 80   | 40    |\n",
    "| 3     | GloryFood | 31.10.2019 | 50     | 5    | 45    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with our test-dataset which contains only data from two zip files\n",
    "tst_filtered_folder = \"./tmp/filtered/\"\n",
    "tst_bs_folder = \"./tmp/bs/\"\n",
    "\n",
    "# folder with the whole dataset as a single parquet\n",
    "all_filtered_folder = \"D:/data/parq_filtered\"\n",
    "all_bs_folder = \"D:/data/parq_bs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22400d32148>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the most important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data doesn't really do anything. It just prepares the df. But we well use the cache() method to keep the data in memory, once it is loaded for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst = spark.read.parquet(tst_filtered_folder).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.read.parquet(all_filtered_folder).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all the contained column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik, adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks, report, line, stmt, inpth, rfile, plabel, negating, ticker, name_cik_tic, exchange, cik_select, "
     ]
    }
   ],
   "source": [
    "_ = [print(x, end=\", \") for x in df_all.columns] # print the name of the columns for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just make a count on the test and the all dataset. This ensure that the data will be loaded into the memory and is cached afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in Test:  1_680_108\n",
      "duration:  13.891040802001953\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Entries in Test: \", \"{:_}\".format(df_tst.count())) # loading test dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in Test:  35_454_045\n",
      "duration:  224.06099796295166\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Entries in Test: \", \"{:_}\".format(df_all.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we filtered out about two thirds of the entries, loading the reduced data set takes only about 3 minutes to load it completely into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test how to pivot the data, we implement a simple example to test the principle. Actually, der is a pivot function, which provides the desired functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+------+------+------+\n",
      "|  company|bs_id|      date|Assets|Cash  |Other |\n",
      "+---------+-----+----------+------+------+------+\n",
      "|VitaSport|    2|31.10.2019|   120|    80|    40|\n",
      "|VitaSport|    1|31.10.2018|   100|    80|    20|\n",
      "|GloryFood|    3|31.10.2019|    50|     5|    45|\n",
      "+---------+-----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bs_data = spark.createDataFrame( \\\n",
    "[ \\\n",
    "    (1,\"VitaSport\",\"31.10.2018\",\"Assets\",100), \\\n",
    "    (1,\"VitaSport\",\"31.10.2018\",\"Cash  \",80 ), \\\n",
    "    (1,\"VitaSport\",\"31.10.2018\",\"Other \",20 ), \\\n",
    "    (2,\"VitaSport\",\"31.10.2019\",\"Assets\",120), \\\n",
    "    (2,\"VitaSport\",\"31.10.2019\",\"Cash  \",80 ), \\\n",
    "    (2,\"VitaSport\",\"31.10.2019\",\"Other \",40 ), \\\n",
    "    (3,\"GloryFood\",\"31.10.2019\",\"Assets\",50 ), \\\n",
    "    (3,\"GloryFood\",\"31.10.2019\",\"Cash  \",5  ), \\\n",
    "    (3,\"GloryFood\",\"31.10.2019\",\"Other \",45 )  \\\n",
    "], \\\n",
    "  (\"bs_id\", \"company\", \"date\", \"attribute\", \"value\") \\\n",
    ")\n",
    "\n",
    "df_bs_data.groupby([\"company\",\"bs_id\",\"date\"]).pivot(\"attribute\").max(\"value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks simple. But it could be, that we will get more than one result. In the above sample, we just used the max aggregate function. However, that might be a too simple solution for real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting Apple in the Testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we select only the BalanceSheet data of Apple in the testset and we expect to have 2 BalanceSheets in there (one for every quarter - since the testset contains two quarter of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_df = df_tst.where(\"cik == 320193 and stmt = 'BS'\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many datarows there are for Apple in the two test quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_vip_cols = apple_df.select(['cik','adsh','period','tag', 'version', 'ddate','uom','value', 'qtrs','fp', 'report','line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+------------+----------+---+----------+----+---+------+----+\n",
      "|   cik|                adsh|    period|                 tag|     version|     ddate|uom|     value|qtrs| fp|report|line|\n",
      "+------+--------------------+----------+--------------------+------------+----------+---+----------+----+---+------+----+\n",
      "|320193|0000320193-19-000119|2019-09-30|  LiabilitiesCurrent|us-gaap/2019|2018-09-30|USD|1.15929E11|   0| FY|     4|  23|\n",
      "|320193|0000320193-19-000119|2019-09-30|  LiabilitiesCurrent|us-gaap/2019|2019-09-30|USD|1.05718E11|   0| FY|     4|  23|\n",
      "|320193|0000320193-19-000119|2019-09-30|ContractWithCusto...|us-gaap/2019|2018-09-30|USD|   5.966E9|   0| FY|     4|  20|\n",
      "|320193|0000320193-19-000119|2019-09-30|ContractWithCusto...|us-gaap/2019|2019-09-30|USD|   5.522E9|   0| FY|     4|  20|\n",
      "|320193|0000320193-19-000119|2019-09-30|              Assets|us-gaap/2019|2019-09-30|USD|3.38516E11|   0| FY|     4|  15|\n",
      "|320193|0000320193-19-000119|2019-09-30|              Assets|us-gaap/2019|2018-09-30|USD|3.65725E11|   0| FY|     4|  15|\n",
      "|320193|0000320193-19-000119|2019-09-30|CommonStocksInclu...|us-gaap/2019|2018-09-30|USD| 4.0201E10|   0| FY|     4|  31|\n",
      "|320193|0000320193-19-000119|2019-09-30|CommonStocksInclu...|us-gaap/2019|2019-09-30|USD| 4.5174E10|   0| FY|     4|  31|\n",
      "|320193|0000320193-19-000119|2019-09-30|OtherAssetsNoncur...|us-gaap/2019|2018-09-30|USD| 2.2283E10|   0| FY|     4|  13|\n",
      "|320193|0000320193-19-000119|2019-09-30|OtherAssetsNoncur...|us-gaap/2019|2019-09-30|USD| 3.2978E10|   0| FY|     4|  13|\n",
      "|320193|0000320193-19-000119|2019-09-30|     CommercialPaper|us-gaap/2019|2018-09-30|USD| 1.1964E10|   0| FY|     4|  21|\n",
      "|320193|0000320193-19-000119|2019-09-30|     CommercialPaper|us-gaap/2019|2019-09-30|USD|    5.98E9|   0| FY|     4|  21|\n",
      "|320193|0000320193-19-000119|2019-09-30|MarketableSecurit...|us-gaap/2019|2018-09-30|USD| 4.0388E10|   0| FY|     4|   4|\n",
      "|320193|0000320193-19-000119|2019-09-30|MarketableSecurit...|us-gaap/2019|2019-09-30|USD| 5.1713E10|   0| FY|     4|   4|\n",
      "|320193|0000320193-19-000119|2019-09-30|LongTermDebtNoncu...|us-gaap/2019|2018-09-30|USD| 9.3735E10|   0| FY|     4|  25|\n",
      "|320193|0000320193-19-000119|2019-09-30|LongTermDebtNoncu...|us-gaap/2019|2019-09-30|USD| 9.1807E10|   0| FY|     4|  25|\n",
      "|320193|0000320193-19-000119|2019-09-30|  StockholdersEquity|us-gaap/2019|2016-09-30|USD|1.28249E11|   0| FY|     4|  34|\n",
      "|320193|0000320193-19-000119|2019-09-30|  StockholdersEquity|us-gaap/2019|2017-09-30|USD|1.34047E11|   0| FY|     4|  34|\n",
      "|320193|0000320193-19-000119|2019-09-30|  StockholdersEquity|us-gaap/2019|2018-09-30|USD|1.07147E11|   0| FY|     4|  34|\n",
      "|320193|0000320193-19-000119|2019-09-30|  StockholdersEquity|us-gaap/2019|2019-09-30|USD| 9.0488E10|   0| FY|     4|  34|\n",
      "+------+--------------------+----------+--------------------+------------+----------+---+----------+----+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_vip_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the \"ddate\" column, we see entries that are in the past (compared to the \"period\" field  - which is the Balance Sheet Date, rounded to nearest month-end). This is normal, since the balancesheet also contains the data of the balance sheet from a year ago. However, in our case we are only interested in the data for the actual period. These are the entries where period and ddate have the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_bs_per_period = apple_vip_cols.where(\"period == ddate\").orderBy([\"cik\",\"adsh\",\"period\",\"report\",\"line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+------------+----------+------+----------+----+---+------+----+\n",
      "|   cik|                adsh|    period|                 tag|     version|     ddate|   uom|     value|qtrs| fp|report|line|\n",
      "+------+--------------------+----------+--------------------+------------+----------+------+----------+----+---+------+----+\n",
      "|320193|0000320193-19-000076|2019-06-30|CashAndCashEquiva...|us-gaap/2018|2019-06-30|   USD|  5.053E10|   0| Q3|     4|   3|\n",
      "|320193|0000320193-19-000076|2019-06-30|MarketableSecurit...|us-gaap/2018|2019-06-30|   USD| 4.4084E10|   0| Q3|     4|   4|\n",
      "|320193|0000320193-19-000076|2019-06-30|AccountsReceivabl...|us-gaap/2018|2019-06-30|   USD| 1.4148E10|   0| Q3|     4|   5|\n",
      "|320193|0000320193-19-000076|2019-06-30|        InventoryNet|us-gaap/2018|2019-06-30|   USD|   3.355E9|   0| Q3|     4|   6|\n",
      "|320193|0000320193-19-000076|2019-06-30|NontradeReceivabl...|us-gaap/2018|2019-06-30|   USD| 1.2326E10|   0| Q3|     4|   7|\n",
      "|320193|0000320193-19-000076|2019-06-30|  OtherAssetsCurrent|us-gaap/2018|2019-06-30|   USD|  1.053E10|   0| Q3|     4|   8|\n",
      "|320193|0000320193-19-000076|2019-06-30|       AssetsCurrent|us-gaap/2018|2019-06-30|   USD|1.34973E11|   0| Q3|     4|   9|\n",
      "|320193|0000320193-19-000076|2019-06-30|MarketableSecurit...|us-gaap/2018|2019-06-30|   USD|1.15996E11|   0| Q3|     4|  11|\n",
      "|320193|0000320193-19-000076|2019-06-30|PropertyPlantAndE...|us-gaap/2018|2019-06-30|   USD| 3.7636E10|   0| Q3|     4|  12|\n",
      "|320193|0000320193-19-000076|2019-06-30|OtherAssetsNoncur...|us-gaap/2018|2019-06-30|   USD| 3.3634E10|   0| Q3|     4|  13|\n",
      "|320193|0000320193-19-000076|2019-06-30|    AssetsNoncurrent|us-gaap/2018|2019-06-30|   USD|1.87266E11|   0| Q3|     4|  14|\n",
      "|320193|0000320193-19-000076|2019-06-30|              Assets|us-gaap/2018|2019-06-30|   USD|3.22239E11|   0| Q3|     4|  15|\n",
      "|320193|0000320193-19-000076|2019-06-30|AccountsPayableCu...|us-gaap/2018|2019-06-30|   USD| 2.9115E10|   0| Q3|     4|  18|\n",
      "|320193|0000320193-19-000076|2019-06-30|OtherLiabilitiesC...|us-gaap/2018|2019-06-30|   USD| 3.1673E10|   0| Q3|     4|  19|\n",
      "|320193|0000320193-19-000076|2019-06-30|ContractWithCusto...|us-gaap/2018|2019-06-30|   USD|   5.434E9|   0| Q3|     4|  20|\n",
      "|320193|0000320193-19-000076|2019-06-30|     CommercialPaper|us-gaap/2018|2019-06-30|   USD|   9.953E9|   0| Q3|     4|  21|\n",
      "|320193|0000320193-19-000076|2019-06-30| LongTermDebtCurrent|us-gaap/2018|2019-06-30|   USD| 1.3529E10|   0| Q3|     4|  22|\n",
      "|320193|0000320193-19-000076|2019-06-30|  LiabilitiesCurrent|us-gaap/2018|2019-06-30|   USD| 8.9704E10|   0| Q3|     4|  23|\n",
      "|320193|0000320193-19-000076|2019-06-30|LongTermDebtNoncu...|us-gaap/2018|2019-06-30|   USD| 8.4936E10|   0| Q3|     4|  25|\n",
      "|320193|0000320193-19-000076|2019-06-30|OtherLiabilitiesN...|us-gaap/2018|2019-06-30|   USD| 5.1143E10|   0| Q3|     4|  26|\n",
      "|320193|0000320193-19-000076|2019-06-30|LiabilitiesNoncur...|us-gaap/2018|2019-06-30|   USD|1.36079E11|   0| Q3|     4|  27|\n",
      "|320193|0000320193-19-000076|2019-06-30|         Liabilities|us-gaap/2018|2019-06-30|   USD|2.25783E11|   0| Q3|     4|  28|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommitmentsAndCon...|us-gaap/2018|2019-06-30|   USD|      null|   0| Q3|     4|  29|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommonStocksInclu...|us-gaap/2018|2019-06-30|   USD| 4.3371E10|   0| Q3|     4|  31|\n",
      "|320193|0000320193-19-000076|2019-06-30|RetainedEarningsA...|us-gaap/2018|2019-06-30|   USD| 5.3724E10|   0| Q3|     4|  32|\n",
      "|320193|0000320193-19-000076|2019-06-30|AccumulatedOtherC...|us-gaap/2018|2019-06-30|   USD|   -6.39E8|   0| Q3|     4|  33|\n",
      "|320193|0000320193-19-000076|2019-06-30|  StockholdersEquity|us-gaap/2018|2019-06-30|   USD| 9.6456E10|   0| Q3|     4|  34|\n",
      "|320193|0000320193-19-000076|2019-06-30|LiabilitiesAndSto...|us-gaap/2018|2019-06-30|   USD|3.22239E11|   0| Q3|     4|  35|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommonStockParOrS...|us-gaap/2018|2019-06-30|   USD|       0.0|   0| Q3|     5|   1|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommonStockShares...|us-gaap/2018|2019-06-30|shares|   1.26E10|   0| Q3|     5|   2|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommonStockShares...|us-gaap/2018|2019-06-30|shares|4.531395E9|   0| Q3|     5|   3|\n",
      "|320193|0000320193-19-000076|2019-06-30|CommonStockShares...|us-gaap/2018|2019-06-30|shares|4.531395E9|   0| Q3|     5|   4|\n",
      "+------+--------------------+----------+--------------------+------------+----------+------+----------+----+---+------+----+\n",
      "only showing top 32 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_bs_per_period.show(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the data above with the BalanceSheet in the appropriate report (https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019319000076/a10-qq320196292019.htm) we see that the data and entries match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we pivot the data and we expect two rows in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_pivoted_df = apple_bs_per_period.select([\"cik\",\"adsh\",\"period\",\"ddate\",'tag','value']) \\\n",
    "                                      .groupby([\"cik\",\"adsh\",\"period\",\"ddate\"]) \\\n",
    "                                      .pivot(\"tag\",['Assets','AssetsCurrent','OtherAssetsCurrent']).max('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+----------+----------+-------------+------------------+\n",
      "|   cik|                adsh|    period|     ddate|    Assets|AssetsCurrent|OtherAssetsCurrent|\n",
      "+------+--------------------+----------+----------+----------+-------------+------------------+\n",
      "|320193|0000320193-19-000119|2019-09-30|2019-09-30|3.38516E11|   1.62819E11|         1.2352E10|\n",
      "|320193|0000320193-19-000076|2019-06-30|2019-06-30|3.22239E11|   1.34973E11|          1.053E10|\n",
      "+------+--------------------+----------+----------+----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple_pivoted_df.select([\"cik\",\"adsh\",\"period\",'ddate', 'Assets','AssetsCurrent','OtherAssetsCurrent']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding which tags to pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis step we created a sorted list of the tags that are present in BalanceSheets. As was shown there, it doesn't make sense to pivot all 3400 tags. Instead, only a small subset appears often enough in reports to be useful. <br>\n",
    "We stored the sorted list in the file \"bs_tags.csv\". No, we will load it and use the first 100 tags to define which values should be pivoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_tags = pd.read_csv(\"./bs_tags.csv\")['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tags = bs_tags[:100].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_bs_ready = df_tst.where(\"stmt = 'BS' and period == ddate\").select(['cik','adsh','period','tag', 'ddate','value']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208895"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bs_ready.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bs_ready.select('tag').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_bs_pivot = df_test_bs_ready.groupby([\"cik\",\"adsh\",\"period\",\"ddate\"]).pivot(\"tag\",relevant_tags) \\\n",
    "                                   .max('value').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5639"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_bs_pivot.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_bs_pivot.write.parquet(tst_bs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_bs_ready = df_all.where(\"stmt = 'BS' and period == ddate\").select(['cik','adsh','period','tag', 'ddate','value']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4720704"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_bs_ready.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2314"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_bs_ready.select('tag').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_bs_pivot = df_all_bs_ready.groupby([\"cik\",\"adsh\",\"period\",\"ddate\"]).pivot(\"tag\",relevant_tags) \\\n",
    "                                 .max('value').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131180"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_bs_pivot.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have an easy way to have a look at the data with a texteditor, we convert it to a pandas Dataframe and store it as CSV. The resulting file size is now 54 MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_bs_pivot.toPandas().to_csv(\"bs_data.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for further processing, we also store it as parquet, since this will keep that datatype information of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(all_bs_folder,  ignore_errors=True)\n",
    "df_all_bs_pivot.repartition(8,col(\"cik\")).write.parquet(all_bs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
