{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data From SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides to code to download the business report data of all companies which is provided by the SEC (US Security and Exchange Commission).\n",
    "There is a zip-file for each quarter since 2009, but only in files starting from 2012 do contain all report data off all companies which reported in that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "import urllib.request  # used to download resources from the web \n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic definitions\n",
    "output_path = \"d:/data/sec_zips\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True) # create directory if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare download urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions to create download urls\n",
    "sec_base_path = \"https://www.sec.gov/files/dera/data/financial-statement-data-sets/\"\n",
    "start_year = 2009        # start year to download the data\n",
    "end_year   = 2020        # end year for download\n",
    "format_str = \"{}q{}.zip\" # all file names are like 2020q1.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with all download links\n",
    "download_urls = []\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for quarter in range(1,5):\n",
    "        download_urls.append(sec_base_path + format_str.format(year, quarter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the file for 2020q1.zip is located at a different location. For what reason ever. Of course, we could parse the site https://www.sec.gov/dera/data/financial-statement-data-sets.html to extract download links directly from there, but since this thesis isn't about parsing and scrapping data from html, we simply add the proper link to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_urls.append(\"https://www.sec.gov/files/node/add/data_distribution/2020q1.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a couple of files, the download should be done in parallel. Of course, basic parallel python packages like \"multiprocessing\" could be used, but since this is a thesis about Spark, Spark should be used to do the things it shines at: parallelize work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ebfce99f48>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using parkContext.parallelize to parallelize with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blog post \"https://medium.com/@joshua_robinson/parallelizing-downloads-with-spark-16bab8e337eb\" shows how the download of resources can be parallelized with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download URL and save to outpath.\n",
    "def downloader(url, outpath):\n",
    "    # From URL construct the destination path and filename.\n",
    "    file_name = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    file_path = os.path.join(outpath, file_name)\n",
    "    \n",
    "    # Check if the file has already been downloaded.\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "    \n",
    "    # Download and write to file.\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=5) as urldata,\\\n",
    "                open(file_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(urldata, out_file)\n",
    "    except Exception as ex:\n",
    "        pass # we cannot really provide a feedback, so we simply ignore failures and assume that these are 404 errors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert URL list to an RDD in order to distribute to workers.\n",
    "# listing = spark.sparkContext.parallelize(download_urls[:2]) # reduce to just two entries for testing\n",
    "listing = spark.sparkContext.parallelize(download_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time:       207.43249559402466\n",
      "size output folder:   1.53GB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "listing.foreach(lambda url: downloader(url, output_path))\n",
    "execution_time = (time.time() - start_time)\n",
    "print(\"execution time:      \", execution_time)\n",
    "print(\"size output folder:  \", get_size_format(get_directory_size(output_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 3.5 minutes to download the 1.53 GB of data on my laptop over my private wlan, connect to my provider Quickline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using user defined functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be possible to use user defined functions and a sparkdataframe with the urls to download the files. Let us find out how this could look. \n",
    "An simple example on how to define a udf can be found here: https://changhsinlee.com/pyspark-udf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the download_urls list has to be converted into a spark dataframe. This can be done with one line, but since we want to have a meaningful columnname, the default columnname \"value\" is changed to \"url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "download_urls_df = spark.createDataFrame(download_urls, StringType())\n",
    "download_urls_df = download_urls_df.withColumnRenamed(\"value\",\"url\")\n",
    "download_urls_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the downloader function is adapted. It now returns also a result and just accepts one parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloader_function(url):\n",
    "    # From URL construct the destination path and filename.\n",
    "    file_name = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    file_path = os.path.join(output_path, file_name) \n",
    "    \n",
    "    # Check if the file has already been downloaded.\n",
    "    if os.path.exists(file_path):\n",
    "        return \"already downloaded\"\n",
    "    \n",
    "    # Download and write to file.\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=30) as urldata,\\\n",
    "                open(file_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(urldata, out_file)\n",
    "            return \"success\"\n",
    "    except Exception as ex:\n",
    "        return \"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the above download function as UDF, it needs to be converted/registered as a udf-function. This is a one liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "downloader_udf = udf(lambda s: downloader_function(s), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the we execute it by simply using an SQL statment, which uses our udf function. With this approach we can also return a result for every url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q3.zip', result='failed'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q4.zip', result='failed')]\n",
      "execution time:       330.34656715393066\n",
      "size output folder:   1.53GB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result_df =  download_urls_df.select('url', downloader_udf('url').alias('result'))\n",
    "result_df_failed = result_df.filter(\"result='failed'\")\n",
    "print(result_df_failed.count())\n",
    "print(result_df_failed.collect())\n",
    "execution_time = (time.time() - start_time)\n",
    "print(\"execution time:      \", execution_time)\n",
    "print(\"size output folder:  \", get_size_format(get_directory_size(output_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, it took 5.5 minutes, but that is more likely due to slower transport of the data over the network than to due to the fact that this version used a UDF.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
