{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_05_Filter_And_Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we filter the lines in which we are interested for the following steps.<br> These are\n",
    "* only lines from a 10K and 10Q reports (field form is \"10-K\" or \"10-Q\")\n",
    "* only lines which belong to a \"primary financal statement\" (field stmt is not empty)\n",
    "* only lines with tags that are not custom tags (version beginning with '00')\n",
    "* only companies whose shares are, or have been traded at NASDAQ or NYSE\n",
    "\n",
    "<br>\n",
    "The result will be stored as a new Parquet-file. We will use our own partition definition in order to make sure, that all lines from one company are inside the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with our test-dataset which contains only data from two zip files\n",
    "tst_parquet_folder = \"./tmp/parquet/\"\n",
    "tst_filtered_folder = \"./tmp/filtered/\"\n",
    "\n",
    "# folder with the whole dataset as a single parquet\n",
    "all_parquet_folder = \"D:/data/parquet/\"\n",
    "all_filtered_folder = \"D:/data/parq_filtered\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24d6c85ef08>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the most important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data doesn't really do anything. It just prepares the df. But we well use the cache() method to keep the data in memory, once it is loaded for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  0.17499494552612305\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_tst = spark.read.parquet(tst_parquet_folder).cache()\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  2.0872995853424072\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_all = spark.read.parquet(all_parquet_folder).cache()\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all the contained column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik, adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks, report, line, stmt, inpth, rfile, plabel, negating, ticker, name_cik_tic, exchange, "
     ]
    }
   ],
   "source": [
    "_ = [print(x, end=\", \") for x in df_all.columns] # print the name of the columns for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just make a count on the test and the all dataset. This ensure that the data will be loaded into the memory and is cached afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in Test:  5_239_639\n"
     ]
    }
   ],
   "source": [
    "print(\"Entries in Test: \", \"{:_}\".format(df_tst.count())) # loading test dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in Test:  109_392_813\n"
     ]
    }
   ],
   "source": [
    "print(\"Entries in Test: \", \"{:_}\".format(df_all.count())) # loading all dataset into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter lines for \"10K\" and \"10Q\" and \"Primary Financial Statement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_string = \"stmt is not null and version NOT LIKE '00%' and form in ('10-K', '10-Q')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_filtered = df_tst.where(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filter   :  2_994_680\n",
      "duration:  4.5779969692230225\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"after filter   : \", \"{:_}\".format(df_tst_filtered.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_filtered = df_all.where(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filter   :  55_696_724\n",
      "duration:  24.144516706466675\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"after filter   : \", \"{:_}\".format(df_all_filtered.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter companies traded at NYSE and NASDAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A companies \"being traded at\" status can change during its life time, so we cannot simply filter by the \"exchange\" column. Instead we have to create a list with all the companies (CIK number) which have been traded or ar traded at NYSE or NASDAQ.\n",
    "<br>\n",
    "We add a column \"cik_select\" which recieves the value 1 which we can use to filter after the join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list with cik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_cik_exchange = df_tst_filtered[['cik','exchange']].distinct() \\\n",
    "    .where(\"exchange in ('NASDAQ','NYSE','NYSE ARCA','NYSE MKT') \").selectExpr(\"cik\", \"1 as cik_select\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count cik_exchange   :  2_844\n",
      "duration:  0.6120038032531738\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"count cik_exchange   : \", \"{:_}\".format(df_tst_cik_exchange.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cik_exchange = df_all_filtered[['cik','exchange']].distinct() \\\n",
    "    .where(\"exchange in ('NASDAQ','NYSE','NYSE ARCA','NYSE MKT') \").selectExpr(\"cik\", \"1 as cik_select\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count cik_exchange   :  4_459\n",
      "duration:  42.441001892089844\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"count cik_exchange   : \", \"{:_}\".format(df_all_cik_exchange.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the cik list with the filtered dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can left join the filtered dataframe with the cik list and use the column \"cik_select\" to filter the ones we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_filter_complete = df_tst_filtered.join(df_tst_cik_exchange, 'cik', \"left\").where(\"cik_select == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count cik_exchange   :  1_680_108\n",
      "duration:  0.8090000152587891\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"count cik_exchange   : \", \"{:_}\".format(df_tst_filter_complete.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_filter_complete = df_all_filtered.join(df_all_cik_exchange, 'cik', \"left\").where(\"cik_select == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count cik_exchange   :  35_454_045\n",
      "duration:  25.643085956573486\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"count cik_exchange   : \", \"{:_}\".format(df_all_filter_complete.count()))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to reduce the 110 million rows to about 35 million rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Save as a new Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we think about defining the partitions, we have to think about how this will effect performance. There are mainly to reasons to use partitions. The first reason ist to be able to read with serveral cores in parallel, so it makes sense to have at least as many partitons as there are cores in the cluster. The second reason is to reduce the amount of data that has to be read. The Parquet-reader can interpret the where-clause of a query and therefore optimize the reading. For instance, let us assume we have data with a country field and use this is the partition key. If we execute a select \"coutry = 'Switzerland'\", then the Parquet-reader will know that it only has to read the partition, in which data for Switzerland are stored.\n",
    "<br>\n",
    "In our case, it makes sense, that all data lines of one company (cik) are stored in the same partition. Moreover, we will use a second level of partitions which are based on the stmt column (the type of the financial statement) since these entries should also belong together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst_partioned = df_tst_filter_complete.repartition(16,col(\"cik\"), col(\"stmt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  17.03229856491089\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(tst_filtered_folder,  ignore_errors=True)\n",
    "start = time.time()\n",
    "df_tst_partioned.write.parquet(tst_filtered_folder)\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_partioned = df_all_filter_complete.repartition(16,col(\"cik\"), col(\"stmt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  238.66271138191223\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(all_filtered_folder,  ignore_errors=True)\n",
    "start = time.time()\n",
    "df_all_partioned.write.parquet(all_filtered_folder)\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
