{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp join_sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_02_Join_SEC_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to join the attributs from the thre files \"num.txt\", \"sub.txt\", and \"pre.txt\" together into one single CSV-file which can then be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic definitions\n",
    "zip_folder = \"./data/\" \n",
    "zip_path = Path(zip_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1462cf8a888>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zip-Files dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in zip_path.glob(\"*.zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the list into a Spark dataframe\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "zip_files_df = spark.createDataFrame(zip_files, StringType())\n",
    "zip_files_df = zip_files_df.withColumnRenamed(\"value\",\"url\")\n",
    "zip_files_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file inside zip and convert it to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the names of the filese inside the zip file\n",
    "SUB_TXT = \"sub.txt\"\n",
    "PRE_TXT = \"pre.txt\"\n",
    "NUM_TXT = \"num.txt\"\n",
    "TAG_TXT = \"tag.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was looking for a way to directly read the content from csv.file inside a zip file into a spark dataframe. But after spending some time researching, i wasn't able to find a way to do it directly.<br>\n",
    "Since that doesn't seem possible, we need to find other solutions and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -> loading an extracted num.txt directly into a Spark dataframe\n",
    "In order to compare the performance of loading csv data into a spark_dataframe we should have a baseline value.<br>\n",
    "We will load the extracted num.txt file from 2019q3 and see how long it will take.\n",
    "Note, the num.txt has to be extracted into the folder \"tmp/2019q3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  0.8069617748260498\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_test_num = spark.read.csv('tmp/2019q3/num.txt', sep='\\t', header=True)\n",
    "print(df_test_num.count()) # we need to execute an action, otherwise only the graph is created\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(adsh='0001625376-19-000017', tag='EntityPublicFloat', version='dei/2014', coreg=None, ddate='20180430', qtrs='0', uom='USD', value='0.0000', footnote=None)\n"
     ]
    }
   ],
   "source": [
    "print(df_test_num.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is pretty reasonable. It took less than a second to load and parse the file into a spark dataframe. (we have to keep in mind, that the disk very likely caches this file after the first load) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract file from zip and load it with spark.csv.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution could be to extract the content and write it as a temporary file and then load that file into a spark dataframe. We cannot use a temporary file (tempfile.TemporaryFile()), since spark will try to access it from another process which is not possible for a temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  2.333956718444824\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "test_zip = zip_files[0]\n",
    "\n",
    "start = time.time()\n",
    "with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "    with container_zip.open(NUM_TXT) as f:\n",
    "        with open(\"./tmp/tempfile.xt\", \"wb+\") as fp:\n",
    "            data = f.read()\n",
    "            fp.write(data)\n",
    "            fp.seek(0)\n",
    "            df_test_num = spark.read.csv(fp.name, sep='\\t', header=True)\n",
    "            print(df_test_num.count())\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it takes a little longer, but it is still a very good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into tuples and create spark dataframe from tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to load the data into a list of tuples and then use that list of tuples to create the spark dataframe. This is code a wrote a few months ago, slightly adapted.<br>\n",
    "This code is not suitable for CSV files containing real text columns, because no escaping is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_fields(row: List[str]) -> Tuple[Union[str,None]]:\n",
    "    \"\"\" This helper method makes sure, that empty entries are converted to None\n",
    "    \"\"\"\n",
    "    return tuple([entry if entry != '' else None for entry in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(zip_file: str, data_file: str) -> Tuple[List[str],List[Tuple[str]]]:\n",
    "    \"\"\" This function extracts the file with the name provided in data_file from a zipfile which name is provided in zip_file.\n",
    "        It then parses the file and returns a list of all tuples.\n",
    "        The function assumes, that there is a header row and that the columns are separated by a \\t.\n",
    "        Furthermore, it assumes that no string escaping has to be done.\n",
    "        \n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            tuple_lines: List[Tuple[str]] = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    line = line.decode(\"utf-8\")\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                    tuple_lines.append(line)\n",
    "                except Exception as ex:\n",
    "                    # sometimes there were encoding problems when storing to windows fs. if utf8 failed, trying to read as\n",
    "                    # as windows-1252 helped in these cases\n",
    "                    try:\n",
    "                        line = line.decode(\"windows-1252\")\n",
    "                        line = line.replace(\"\\n\", \"\")\n",
    "                        line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                        tuple_lines.append(line)\n",
    "                    except:\n",
    "                        sys.stderr.write(str(ex), \"   \", line)\n",
    "            return list(tuple_lines[:1][0]), tuple_lines[1:] # skip the header row, since we know that all files that we read have a header row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the code above and to have feeling for the performance, we measure the time that is needed to load the num.txt file directly from the zip file and convert it into a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsh : 0000034563-19-000064\n",
      "duration:  8.604996681213379\n"
     ]
    }
   ],
   "source": [
    "# A short check to see if the reading works\n",
    "start = time.time()\n",
    "headers, list_of_tuples = get_file_data(zip_files[0], NUM_TXT)\n",
    "print(headers[0],\":\", list_of_tuples[1][0])\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 9 seconds. <br>\n",
    "Just creating the list with tuples is already much slower than extracting the file and using spark.read.csv. Lets check how long the creation of a spark dataframe out of the tuple will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  132.30002903938293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "headers, list_of_tuples = get_file_data(zip_files[0], NUM_TXT)\n",
    "df_tuple = spark.createDataFrame(list_of_tuples , headers)\n",
    "print(df_tuple.count())\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes over 2 minutes. Of course, it maybe that there are better ways to do it, but since the performance of reading directly from a file performs way better, it doesn't make sense to try to follow this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spark.read.csv with RDD parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  13.77500033378601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "start = time.time()\n",
    "with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "    with container_zip.open(NUM_TXT) as f:\n",
    "        lines = [line.decode(\"utf-8\") for line in f.readlines()]\n",
    "        df_test_num = spark.read.csv(spark.sparkContext.parallelize(lines), sep='\\t', header=True)\n",
    "        print(df_test_num.count())        \n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh',\n",
       " 'tag',\n",
       " 'version',\n",
       " 'coreg',\n",
       " 'ddate',\n",
       " 'qtrs',\n",
       " 'uom',\n",
       " 'value',\n",
       " 'footnote']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_num.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takeslonger than loading the file directly. But it would be a very easy to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Since this is mainly a ontime operation, i will use the \"Using spark.read.csv with RDD parallelize\" as a first approach. If that shouldn't work out well, i would go for the extract and save to disk approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Reader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_zip_into_df(zip_file: str, data_file: str) -> DataFrame:\n",
    "    with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "        with container_zip.open(NUM_TXT) as f:\n",
    "            lines = [line.decode(\"utf-8\") for line in f.readlines()]\n",
    "            df = spark.read.csv(spark.sparkContext.parallelize(lines), sep='\\t', header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the data into one spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lap1:  2.0500004291534424\n",
      "lap2:  66.70102596282959\n",
      "lap2:  133.26235961914062\n"
     ]
    }
   ],
   "source": [
    "# this takes some time till loaded\n",
    "# lap1:  2.0500004291534424\n",
    "# lap2:  66.70102596282959\n",
    "# lap2:  133.26235961914062 -> loading the data and creating a tuple is only about 8 seconds... so about 2 minutes are needed to create the df from the tuple\n",
    "start = time.time()\n",
    "df_sub = read_csv_in_zip_into_df(zip_files[0], SUB_TXT)\n",
    "lap1 = time.time()\n",
    "lap1_time = lap1-start\n",
    "print(\"lap1: \", lap1_time)\n",
    "df_pre = read_csv_in_zip_into_df(zip_files[0], PRE_TXT)\n",
    "lap2 = time.time()\n",
    "lap2_time = lap2-lap1\n",
    "print(\"lap2: \", lap2_time)\n",
    "df_num = read_csv_in_zip_into_df(zip_files[0], NUM_TXT)\n",
    "lap3 = time.time()\n",
    "lap3_time = lap3-lap2\n",
    "print(\"lap2: \", lap3_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh', 'adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this join produces a df with two duplicated columns named adsh\n",
    "# that should be prevented: https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html\n",
    "df_join1 = df_num.join(df_sub, df_num.adsh == df_sub.adsh)\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh is twice in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct way of joining using a list with the column names\n",
    "df_join1 = df_num.join(df_sub, [\"adsh\"])\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh appears now only once in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the dataframes together\n",
    "df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"version\",\"tag\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2570409"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.count() # this will start the whole DAG and executes the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\2019q3.zip'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
