{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp join_sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_02_Join_SEC_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to join the attributs from the thre files \"num.txt\", \"sub.txt\", and \"pre.txt\" together into one single CSV-file which can then be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic definitions\n",
    "zip_folder = \"./data/\" \n",
    "zip_path = Path(zip_folder)\n",
    "\n",
    "extract_temp_folder = \"./tmp/extract/\"\n",
    "Path(extract_temp_folder).mkdir(parents=True, exist_ok=True) # create directory if necessary\n",
    "\n",
    "target_folder = \"./tmp/joined/\"\n",
    "Path(target_path).mkdir(parents=True, exist_ok=True) # create directory if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25a1718bfc8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zip-Files dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in zip_path.glob(\"*.zip\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file inside zip and convert it to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the names of the filese inside the zip file\n",
    "SUB_TXT = \"sub.txt\"\n",
    "PRE_TXT = \"pre.txt\"\n",
    "NUM_TXT = \"num.txt\"\n",
    "TAG_TXT = \"tag.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was looking for a way to directly read the content from csv.file inside a zip file into a spark dataframe. But after spending some time researching, i wasn't able to find a way to do it directly.<br>\n",
    "Since that doesn't seem possible, we need to find other solutions and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline -> loading an extracted num.txt directly into a Spark dataframe\n",
    "In order to compare the performance of loading csv data into a spark_dataframe we should have a baseline value.<br>\n",
    "We will load the extracted num.txt file from 2019q3 and see how long it will take.\n",
    "Note, the num.txt has to be extracted into the folder \"tmp/2019q3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  1.0419988632202148\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_test_num = spark.read.csv('tmp/2019q3/num.txt', sep='\\t', header=True)\n",
    "print(df_test_num.count()) # we need to execute an action, otherwise only the graph is created\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(adsh='0001625376-19-000017', tag='EntityPublicFloat', version='dei/2014', coreg=None, ddate='20180430', qtrs='0', uom='USD', value='0.0000', footnote=None)\n"
     ]
    }
   ],
   "source": [
    "print(df_test_num.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is pretty reasonable. It took less than a second to load and parse the file into a spark dataframe. (we have to keep in mind, that the disk very likely caches this file after the first load, so it should be called twice) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: Extract file from zip and load it with spark.csv.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution could be to extract the content and write it as a temporary file and then load that file into a spark dataframe. We cannot use a temporary file (tempfile.TemporaryFile()), since spark will try to access it from another process which is not possible for a temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data2019q3'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_zip.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  2.1444883346557617\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "test_zip = zip_files[0]\n",
    "data_file = NUM_TXT\n",
    "\n",
    "start = time.time()\n",
    "with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "    with container_zip.open(data_file) as f:\n",
    "        tempfile = extract_temp_folder + test_zip.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")+\"_\"+data_file\n",
    "        with open(tempfile, \"wb+\") as fp:\n",
    "            data = f.read()\n",
    "            fp.write(data)\n",
    "            fp.seek(0)\n",
    "            df_test_num = spark.read.csv(fp.name, sep='\\t', header=True)\n",
    "            print(df_test_num.count())\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it takes a little longer, but it is still a very good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: Using spark.read.csv with RDD parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  14.239033222198486\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "start = time.time()\n",
    "with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "    with container_zip.open(NUM_TXT) as f:\n",
    "        lines = [line.decode(\"utf-8\") for line in f.readlines()]\n",
    "        df_test_num = spark.read.csv(spark.sparkContext.parallelize(lines), sep='\\t', header=True)\n",
    "        print(df_test_num.count())        \n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takeslonger than loading the file directly. But it is easy to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3: Load data into tuples and create spark dataframe from tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to load the data into a list of tuples and then use that list of tuples to create the spark dataframe. This is code a wrote a few months ago, slightly adapted.<br>\n",
    "This code is not suitable for CSV files containing real text columns, because no escaping is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_fields(row: List[str]) -> Tuple[Union[str,None]]:\n",
    "    \"\"\" This helper method makes sure, that empty entries are converted to None\n",
    "    \"\"\"\n",
    "    return tuple([entry if entry != '' else None for entry in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(zip_file: str, data_file: str) -> Tuple[List[str],List[Tuple[str]]]:\n",
    "    \"\"\" This function extracts the file with the name provided in data_file from a zipfile which name is provided in zip_file.\n",
    "        It then parses the file and returns a list of all tuples.\n",
    "        The function assumes, that there is a header row and that the columns are separated by a \\t.\n",
    "        Furthermore, it assumes that no string escaping has to be done.\n",
    "        \n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            tuple_lines: List[Tuple[str]] = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    line = line.decode(\"utf-8\")\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                    tuple_lines.append(line)\n",
    "                except Exception as ex:\n",
    "                    # sometimes there were encoding problems when storing to windows fs. if utf8 failed, trying to read as\n",
    "                    # as windows-1252 helped in these cases\n",
    "                    try:\n",
    "                        line = line.decode(\"windows-1252\")\n",
    "                        line = line.replace(\"\\n\", \"\")\n",
    "                        line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                        tuple_lines.append(line)\n",
    "                    except:\n",
    "                        sys.stderr.write(str(ex), \"   \", line)\n",
    "            return list(tuple_lines[:1][0]), tuple_lines[1:] # skip the header row, since we know that all files that we read have a header row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the code above and to have feeling for the performance, we measure the time that is needed to load the num.txt file directly from the zip file and convert it into a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsh : 0000034563-19-000064\n",
      "duration:  10.322304248809814\n"
     ]
    }
   ],
   "source": [
    "# A short check to see if the reading works\n",
    "start = time.time()\n",
    "headers, list_of_tuples = get_file_data(zip_files[0], NUM_TXT)\n",
    "print(headers[0],\":\", list_of_tuples[1][0])\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 9 seconds. <br>\n",
    "Just creating the list with tuples is already much slower than extracting the file and using spark.read.csv. Lets check how long the creation of a spark dataframe out of the tuple will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325267\n",
      "duration:  132.30002903938293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "headers, list_of_tuples = get_file_data(zip_files[0], NUM_TXT)\n",
    "df_tuple = spark.createDataFrame(list_of_tuples , headers)\n",
    "print(df_tuple.count())\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes over 2 minutes. Of course, it maybe that there are better ways to do it, but since the performance of reading directly from a file performs way better, it doesn't make sense to try to follow this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Since extracting the file and reading it directly with the read.csv method performs the best, we will use this approach. We have to pay attention, that we create unique temporary files and that the clean them once the job is finished. So we will extract them in its own folder \"extract_temp_folder\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Reader implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1: Function: Extract the file into a temporary file and use spark.read.csv directly with the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method will be the method we are going to use in the final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_zip_into_df_extract(zip_file: str, data_file: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "       Extracts the data from zipfile and stores it on disk. \n",
    "       Uses spark.csv.read to read the data into the df\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            # create a unique tempfile to extract the data\n",
    "            tempfile = extract_temp_folder + zip_file.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")+\"_\"+data_file\n",
    "            with open(tempfile, \"wb+\") as fp:\n",
    "                data = f.read()\n",
    "                fp.write(data)\n",
    "                fp.seek(0)\n",
    "                df = spark.read.csv(fp.name, sep='\\t', header=True)\n",
    "                return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2: Function: Reading directly from Zip into RDD and create Dataframe from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the overall performance, we implement also the method who reads the data directly from the zip into memory and uses RDDs to create the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_zip_into_df_direct(zip_file: str, data_file: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "       extracting the data directly from zipfile into the memory. we need to call decode with utf-8. \n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(test_zip, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            lines = [line.decode(\"utf-8\") for line in f.readlines()]\n",
    "            df = spark.read.csv(spark.sparkContext.parallelize(lines), sep='\\t', header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the data into one spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the spark dataframe's join method to join the data and we will compare the performance between to variants descriped above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining joins, we have to pay attention to define them correctly if the join columns have the same name in both dataframes. Otherwise it can happen, that the join columns appear twice in the resulting dataframe.<br>\n",
    "Details can be found here: https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh', 'adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this join produces a df with two duplicated columns named adsh\n",
    "# that should be prevented: \n",
    "df_join1 = df_num.join(df_sub, df_num.adsh == df_sub.adsh)\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh is twice in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct way of joining using a list with the column names\n",
    "df_join1 = df_num.join(df_sub, [\"adsh\"])\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh appears now only once in the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: Extract the file into a temporary file and use spark.read.csv directly with the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just creates and prepares the graph, nothing happens yet\n",
    "df_sub = read_csv_in_zip_into_df_extract(zip_files[0], SUB_TXT)\n",
    "df_pre = read_csv_in_zip_into_df_extract(zip_files[0], PRE_TXT)\n",
    "df_num = read_csv_in_zip_into_df_extract(zip_files[0], NUM_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  2570409\n",
      "duration:  6.892997980117798\n"
     ]
    }
   ],
   "source": [
    "# joining is the same as above\n",
    "start = time.time()\n",
    "df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"tag\",\"version\"],\"left\")\n",
    "print(\"count: \", df_joined.count()) # again, calling count to ensure that the df is completely initialized\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result speaks for itself. Only 7 seconds were used to load the data and join it. Also here, during these 7 seconds, almost 100% of the availabe CPU power was used on all cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TaskManager_Extract.png](./images/TaskManager_Extract.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: Reading directly from Zip into RDD and create Dataframe from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just creates and prepares the graph, nothing happens yet\n",
    "df_sub = read_csv_in_zip_into_df_direct(zip_files[0], SUB_TXT)\n",
    "df_pre = read_csv_in_zip_into_df_direct(zip_files[0], PRE_TXT)\n",
    "df_num = read_csv_in_zip_into_df_direct(zip_files[0], NUM_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells defines the join. <br>\n",
    "df_num and df_sub have to be joined based on attribut \"adsh\". We can use an inner join for that since we know that every entry in num has a reference in sub. The result of this join is then joined with pre. We use a left outer join for that, since not every num must have an entry in pre. Since it is possible that there is more than one entry in pre for a pre, the total number of records will be likely larger than the rows in num alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  2570409\n",
      "duration:  40.54203271865845\n"
     ]
    }
   ],
   "source": [
    "# joining the dataframes together\n",
    "start = time.time()\n",
    "df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"tag\",\"version\"],\"left\")\n",
    "print(\"count: \", df_joined.count()) # again, calling count to ensure that the df is completely initialized\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Execute the whole graph, it took about 40 seconds.<br>\n",
    "Checking the Windows TaskManager, we see that almost 100% CPU was used on all cores to execute the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TaskManager_RDD.png](./images/TaskManager_RDD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glueing it together and adding writing as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(zip_file: str, target_folder: str) -> str:\n",
    "    \"\"\"\n",
    "        Joins the content of the 3 csv files that are contained in the zip file and \n",
    "        create on csv file containing all relevant columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_path = target_folder + Path(zip_file).name.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
    "    \n",
    "    df_sub = read_csv_in_zip_into_df_extract(zip_file, SUB_TXT)\n",
    "    df_pre = read_csv_in_zip_into_df_extract(zip_file, PRE_TXT)\n",
    "    df_num = read_csv_in_zip_into_df_extract(zip_file, NUM_TXT)\n",
    "    \n",
    "    df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"tag\",\"version\"],\"left\")\n",
    "    \n",
    "    df_joined.write.csv(target_path, compression=\"gzip\", header=True)\n",
    "    \n",
    "    return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/joined/data2019q3\n",
      "duration:  65.65727496147156\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(join_files(zip_files[0], target_folder))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 60 seconds, which is a little surprising, since a count on the joined df only took about 7 seconds. So the question is if this is just the writing or were there other optimisation for count possible and which explain the difference between the \"V1: extract to file\" and \"V2: Reading directly from Zip into RDD\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: Final Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if this version is also slower when data is written into a csv, let us adapt the code and use the direct \"rdd\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files_rdd(zip_file: str) -> str:\n",
    "    \"\"\"\n",
    "        Joins the content of the 3 csv files that are contained in the zip file and \n",
    "        create on csv file containing all relevant columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_path = target_folder + Path(zip_file).name.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
    "    \n",
    "    df_sub = read_csv_in_zip_into_df_direct(zip_file, SUB_TXT)\n",
    "    df_pre = read_csv_in_zip_into_df_direct(zip_file, PRE_TXT)\n",
    "    df_num = read_csv_in_zip_into_df_direct(zip_file, NUM_TXT)\n",
    "    \n",
    "    df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"tag\",\"version\"],\"left\")\n",
    "    \n",
    "    df_joined.write.csv(target_path, compression=\"gzip\", header=True)\n",
    "    \n",
    "    return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/joined/data2019q3\n",
      "duration:  96.1389946937561\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(join_files_rdd(zip_files[0]))\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that the RDD version (V2) is slower and we will stick with the extract version (V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on all 46 zipfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final test and to get a baseline, we will run the \"joining\" an all 46 zip files.<br>\n",
    "Since the processing of every single zip file already uses all cores very much in parallel, there is no need to try to parallize the processing of multpiple zip files at the same time. So this will be just one loop which processes the files sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zip_file_folder = \"D:/data/sec_zips/\"\n",
    "all_zip_path = Path(all_zip_file_folder)\n",
    "zip_files = [str(file) for file in all_zip_path.glob(\"*.zip\")]\n",
    "\n",
    "target = \"d:/data/zip_joined/\"\n",
    "Path(target).mkdir(parents=True, exist_ok=True) # create directory if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "D:\\data\\sec_zips\\2009q1.zip\n",
      "d:/data/zip_joined/2009q1\n",
      "D:\\data\\sec_zips\\2009q2.zip\n",
      "d:/data/zip_joined/2009q2\n",
      "D:\\data\\sec_zips\\2009q3.zip\n",
      "d:/data/zip_joined/2009q3\n",
      "D:\\data\\sec_zips\\2009q4.zip\n",
      "d:/data/zip_joined/2009q4\n",
      "D:\\data\\sec_zips\\2010q1.zip\n",
      "d:/data/zip_joined/2010q1\n",
      "D:\\data\\sec_zips\\2010q2.zip\n",
      "d:/data/zip_joined/2010q2\n",
      "D:\\data\\sec_zips\\2010q3.zip\n",
      "d:/data/zip_joined/2010q3\n",
      "D:\\data\\sec_zips\\2010q4.zip\n",
      "d:/data/zip_joined/2010q4\n",
      "D:\\data\\sec_zips\\2011q1.zip\n",
      "d:/data/zip_joined/2011q1\n",
      "D:\\data\\sec_zips\\2011q2.zip\n",
      "d:/data/zip_joined/2011q2\n",
      "D:\\data\\sec_zips\\2011q3.zip\n",
      "d:/data/zip_joined/2011q3\n",
      "D:\\data\\sec_zips\\2011q4.zip\n",
      "d:/data/zip_joined/2011q4\n",
      "D:\\data\\sec_zips\\2012q1.zip\n",
      "d:/data/zip_joined/2012q1\n",
      "D:\\data\\sec_zips\\2012q2.zip\n",
      "d:/data/zip_joined/2012q2\n",
      "D:\\data\\sec_zips\\2012q3.zip\n",
      "d:/data/zip_joined/2012q3\n",
      "D:\\data\\sec_zips\\2012q4.zip\n",
      "d:/data/zip_joined/2012q4\n",
      "D:\\data\\sec_zips\\2013q1.zip\n",
      "d:/data/zip_joined/2013q1\n",
      "D:\\data\\sec_zips\\2013q2.zip\n",
      "d:/data/zip_joined/2013q2\n",
      "D:\\data\\sec_zips\\2013q3.zip\n",
      "d:/data/zip_joined/2013q3\n",
      "D:\\data\\sec_zips\\2013q4.zip\n",
      "d:/data/zip_joined/2013q4\n",
      "D:\\data\\sec_zips\\2014q1.zip\n",
      "d:/data/zip_joined/2014q1\n",
      "D:\\data\\sec_zips\\2014q2.zip\n",
      "d:/data/zip_joined/2014q2\n",
      "D:\\data\\sec_zips\\2014q3.zip\n",
      "d:/data/zip_joined/2014q3\n",
      "D:\\data\\sec_zips\\2014q4.zip\n",
      "d:/data/zip_joined/2014q4\n",
      "D:\\data\\sec_zips\\2015q1.zip\n",
      "d:/data/zip_joined/2015q1\n",
      "D:\\data\\sec_zips\\2015q2.zip\n",
      "d:/data/zip_joined/2015q2\n",
      "D:\\data\\sec_zips\\2015q3.zip\n",
      "d:/data/zip_joined/2015q3\n",
      "D:\\data\\sec_zips\\2015q4.zip\n",
      "d:/data/zip_joined/2015q4\n",
      "D:\\data\\sec_zips\\2016q1.zip\n",
      "d:/data/zip_joined/2016q1\n",
      "D:\\data\\sec_zips\\2016q2.zip\n",
      "d:/data/zip_joined/2016q2\n",
      "D:\\data\\sec_zips\\2016q3.zip\n",
      "d:/data/zip_joined/2016q3\n",
      "D:\\data\\sec_zips\\2016q4.zip\n",
      "d:/data/zip_joined/2016q4\n",
      "D:\\data\\sec_zips\\2017q1.zip\n",
      "d:/data/zip_joined/2017q1\n",
      "D:\\data\\sec_zips\\2017q2.zip\n",
      "d:/data/zip_joined/2017q2\n",
      "D:\\data\\sec_zips\\2017q3.zip\n",
      "d:/data/zip_joined/2017q3\n",
      "D:\\data\\sec_zips\\2017q4.zip\n",
      "d:/data/zip_joined/2017q4\n",
      "D:\\data\\sec_zips\\2018q1.zip\n",
      "d:/data/zip_joined/2018q1\n",
      "D:\\data\\sec_zips\\2018q2.zip\n",
      "d:/data/zip_joined/2018q2\n",
      "D:\\data\\sec_zips\\2018q3.zip\n",
      "d:/data/zip_joined/2018q3\n",
      "D:\\data\\sec_zips\\2018q4.zip\n",
      "d:/data/zip_joined/2018q4\n",
      "D:\\data\\sec_zips\\2019q1.zip\n",
      "d:/data/zip_joined/2019q1\n",
      "D:\\data\\sec_zips\\2019q2.zip\n",
      "d:/data/zip_joined/2019q2\n",
      "D:\\data\\sec_zips\\2019q3.zip\n",
      "d:/data/zip_joined/2019q3\n",
      "D:\\data\\sec_zips\\2019q4.zip\n",
      "d:/data/zip_joined/2019q4\n",
      "D:\\data\\sec_zips\\2020q1.zip\n",
      "d:/data/zip_joined/2020q1\n",
      "D:\\data\\sec_zips\\2020q2.zip\n",
      "d:/data/zip_joined/2020q2\n",
      "duration:  2598.309921503067\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(len(zip_files))\n",
    "for file in zip_files:\n",
    "    try: \n",
    "        print(file)\n",
    "        print(join_files(file, target))\n",
    "    except Exception as ex:\n",
    "        print(\"failed: \", file, str(ex))\n",
    "duration = time.time() - start\n",
    "\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 2600 seconds (about 43 min), all files were processed and there was no exception. All content in the joined folder has a total size of about 5.5GB.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear extract folder\n",
    "shutil.rmtree(extract_temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
