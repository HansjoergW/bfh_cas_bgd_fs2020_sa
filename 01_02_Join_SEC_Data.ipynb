{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp join_sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_02_Join_SEC_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to join the attributs from the thre files \"num.txt\", \"sub.txt\", and \"pre.txt\" together into one single CSV-file which can then be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic definitions\n",
    "zip_folder = \"./data/\" \n",
    "zip_path = Path(zip_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20e91c988c8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zip-Files dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in zip_path.glob(\"*.zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the list into a Spark dataframe\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "zip_files_df = spark.createDataFrame(zip_files, StringType())\n",
    "zip_files_df = zip_files_df.withColumnRenamed(\"value\",\"url\")\n",
    "zip_files_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file inside zip and convert it to a spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was looking for a way to directly read the content from csv.file inside a zip file into a spark dataframe. But after spending some time researching, i wasn't able to find a way to do it directly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution could be to extract the content and write it as a temporary file and then load that file into a spark dataframe. But spark does the loading in the background, which means, as soon as the cell is finished, the tempfile gets deleted and the spark task fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to load the data into a list of tuples and then use that list of tuples to create the spark dataframe. This is code a wrote a few months ago, slightly adapted.<br>\n",
    "This code is not suitable for CSV files containing real text columns, because no escaping is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_fields(row: List[str]) -> Tuple[Union[str,None]]:\n",
    "    \"\"\" This helper method makes sure, that empty entries are converted to None\n",
    "    \"\"\"\n",
    "    return tuple([entry if entry != '' else None for entry in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(zip_file: str, data_file: str) -> Tuple[List[str],List[Tuple[str]]]:\n",
    "    \"\"\" This function extracts the file with the name provided in data_file from a zipfile which name is provided in zip_file.\n",
    "        It then parses the file and returns a list of all tuples.\n",
    "        The function assumes, that there is a header row and that the columns are separated by a \\t.\n",
    "        Furthermore, it assumes that no string escaping has to be done.\n",
    "        \n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            tuple_lines: List[Tuple[str]] = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    line = line.decode(\"utf-8\")\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                    tuple_lines.append(line)\n",
    "                except Exception as ex:\n",
    "                    # sometimes there were encoding problems when storing to windows fs. if utf8 failed, trying to read as\n",
    "                    # as windows-1252 helped in these cases\n",
    "                    try:\n",
    "                        line = line.decode(\"windows-1252\")\n",
    "                        line = line.replace(\"\\n\", \"\")\n",
    "                        line = clear_empty_fields(line.split(\"\\t\"))\n",
    "                        tuple_lines.append(line)\n",
    "                    except:\n",
    "                        sys.stderr.write(str(ex), \"   \", line)\n",
    "            return list(tuple_lines[:1][0]), tuple_lines[1:] # skip the header row, since we know that all files that we read have a header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsh : 0000002488-19-000104\n"
     ]
    }
   ],
   "source": [
    "# A short check to see if the reading works\n",
    "headers, list_of_tuples = get_file_data(zip_files[0], \"sub.txt\")\n",
    "print(headers[0],\":\", list_of_tuples[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_zip_into_df(zip_file: str, data_file: str) -> DataFrame:\n",
    "    headers, list_of_tuples = get_file_data(zip_file, data_file)\n",
    "    return spark.createDataFrame(list_of_tuples , headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, adsh: string, cik: string, name: string, sic: string, countryba: string, stprba: string, cityba: string, zipba: string, bas1: string, bas2: string, baph: string, countryma: string, stprma: string, cityma: string, zipma: string, mas1: string, mas2: string, countryinc: string, stprinc: string, ein: string, former: string, changed: string, afs: string, wksi: string, fye: string, form: string, period: string, fy: string, fp: string, filed: string, accepted: string, prevrpt: string, detail: string, instance: string, nciks: string, aciks: string]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if it is working\n",
    "df = read_csv_in_zip_into_df(zip_files[0], \"sub.txt\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the names of the filese inside the zip file\n",
    "SUB_TXT = \"sub.txt\"\n",
    "PRE_TXT = \"pre.txt\"\n",
    "NUM_TXT = \"num.txt\"\n",
    "TAG_TXT = \"tag.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes some time till loaded\n",
    "df_sub = read_csv_in_zip_into_df(zip_files[0], SUB_TXT)\n",
    "df_pre = read_csv_in_zip_into_df(zip_files[0], PRE_TXT)\n",
    "df_num = read_csv_in_zip_into_df(zip_files[0], NUM_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh', 'adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this join produces a df with two duplicated columns named adsh\n",
    "# that should be prevented: https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html\n",
    "df_join1 = df_num.join(df_sub, df_num.adsh == df_sub.adsh)\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh is twice in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsh']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct way of joining using a list with the column names\n",
    "df_join1 = df_num.join(df_sub, [\"adsh\"])\n",
    "[x for x in df_join1.columns if x == \"adsh\"] # shows that the column adsh appears now only once in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
