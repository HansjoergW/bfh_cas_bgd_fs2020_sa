{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00_01_Parquet_Basics\n",
    "Notebook to explore the basics of Parquet\n",
    "Links\n",
    "* Spark documentation https://spark.apache.org/docs/2.4.5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import os\n",
    "import zipfile\n",
    "from bfh_cas_bgd_fs2020_sa.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ieu\\projects\\bfh_cas_bgd_fs2020_sa\n"
     ]
    }
   ],
   "source": [
    "# Check the current working dir\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Definition\n",
    "data_folder = \"./data/\" # folder with testdata\n",
    "temp_folder = \"./tmp/\"\n",
    "parquet_folder = \"./parquet/\"\n",
    "data_files = ['2019q3.zip','2019q4.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark\n",
    "This code initialises the SparkSession and therefore the SparkContext. Pressing the link \"Spark UI\" opens the Spark UI for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x217a271d0c8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datafiles\n",
    "The directory contains two zipfiles (2019q3.zip, 2019q4.zip). Each of them contains 4 csv files. The columns and the relation between these files are described in the readme.htm.<br>\n",
    "Each zip file contains all quarterly and yearly reports that were filled during the quarter denoted by the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking\n",
    "In a first step, the content of the files are unzipped and placed in separated folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_file in data_files:\n",
    "    path_to_zip_file = data_folder + data_file\n",
    "    directory_to_extract_to = temp_folder + data_file[:-4]\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sizes of the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder:  76.76MB\n",
      "Temp folder:  748.04MB\n"
     ]
    }
   ],
   "source": [
    "print('Data folder: ', get_size_format(get_directory_size(data_folder)))\n",
    "print('Temp folder: ', get_size_format(get_directory_size(temp_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Parquet\n",
    "let us read a csv file and store it as a parquet file\n",
    "* API doc of the csv reader: https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html?highlight=parquet#pyspark.sql.DataFrameReader.csv\n",
    "* API doc of the parquet writer: https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports that are used in this section\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import countDistinct, year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test file:  236.70MB\n"
     ]
    }
   ],
   "source": [
    "# as a test csv file, \"num.txt\" from the folder 2019q3 is used\n",
    "test_file = temp_folder + '2019q3/num.txt'\n",
    "print('size of test file: ', get_size_format(os.path.getsize(test_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a CSV file\n",
    "As a first step, the csv file has to be loaded into a spark df. <br>\n",
    "The file has a header row and the columns are separated by a TAB (\\t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_num = spark.read.csv(test_file, sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking the format in the next cell, we see that all columns were read as a string. That is ok for most of the columns but when checking the definitions for the num.txt file in the readme.htm we see, that ddate is a date in the format 'yyyymmdd', qtrs and coreg are 'int' and value is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first row:       [Row(adsh='0001625376-19-000017', tag='EntityPublicFloat', version='dei/2014', coreg=None, ddate='20180430', qtrs='0', uom='USD', value='0.0000', footnote=None)]\n",
      "number of rows : 2325267\n",
      "root\n",
      " |-- adsh: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- coreg: string (nullable = true)\n",
      " |-- ddate: string (nullable = true)\n",
      " |-- qtrs: string (nullable = true)\n",
      " |-- uom: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- footnote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('first row:      ', df_test_num.head(1))\n",
    "print('number of rows :', df_test_num.count())\n",
    "df_test_num.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT adsh)|\n",
      "+--------------------+\n",
      "|                6283|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show how many different reports are available in this quarter\n",
    "df2 = df_test_num.select(countDistinct(\"adsh\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acutally, spark can try to infer the types of the columns from the data itself, so lets try that by using the \"inferSchema\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_num = spark.read.csv(test_file, sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the next cell, we were only partially sucessfull. The reader was able to detect that qtrs is an integer and that value is a double. But it failed to recognize that ddate is actually a date and that coreg should be an int. That was to be expected: ddate looks like an int and the coreg field is only used in special situations, so there is a good change that its content is None for all entries in the file. <br>\n",
    "It looks as if we have to define the schema by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(adsh='0001625376-19-000017', tag='EntityPublicFloat', version='dei/2014', coreg=None, ddate=20180430, qtrs=0, uom='USD', value=0.0, footnote=None)]\n",
      "root\n",
      " |-- adsh: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- coreg: string (nullable = true)\n",
      " |-- ddate: integer (nullable = true)\n",
      " |-- qtrs: integer (nullable = true)\n",
      " |-- uom: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_test_num.head(1))\n",
    "df_test_num.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary classes to define a schema are located inside the package pyspark.sql.types and for our example we need the following import<br>\n",
    "```from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType```\n",
    "\n",
    "An important point is that the dateFormat has to be defined as parameter when calling spark.read.csv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"adsh\",    StringType(),  True),\\\n",
    "                     StructField(\"tag\",     StringType(),  True),\\\n",
    "                     StructField(\"version\", StringType(),  True),\\\n",
    "                     StructField(\"coreg\",   IntegerType(), True),\\\n",
    "                     StructField(\"ddate\",   DateType(),    True),\\\n",
    "                     StructField(\"qtrs\",    IntegerType(), True),\\\n",
    "                     StructField(\"uom\",     StringType(),  True),\\\n",
    "                     StructField(\"value\",   DoubleType(),  True),\\\n",
    "                     StructField(\"footnote\",StringType(),  True)\\\n",
    "                    ])\n",
    "df_test_num = spark.read.csv(test_file, sep='\\t', header=True, dateFormat=\"yyyyMMdd\", schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(adsh='0001625376-19-000017', tag='EntityPublicFloat', version='dei/2014', coreg=None, ddate=datetime.date(2018, 4, 30), qtrs=0, uom='USD', value=0.0, footnote=None)]\n",
      "root\n",
      " |-- adsh: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- coreg: integer (nullable = true)\n",
      " |-- ddate: date (nullable = true)\n",
      " |-- qtrs: integer (nullable = true)\n",
      " |-- uom: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_test_num.head(1))\n",
    "df_test_num.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple write as Parquet\n",
    "As first version the dataframe is stored directly in parquet format without additional options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder_pure = parquet_folder+\"pure/\"\n",
    "df_test_num.write.parquet(parquet_folder_pure, mode=\"overwrite\") # mode 'overwrite' overwrites the data, if they are already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of parquet_folder_pure:  19.11MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.part-00000-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00001-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00002-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00003-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00004-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00005-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00006-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '.part-00007-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00001-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00002-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00003-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00004-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00005-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00006-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " 'part-00007-a86b89ce-d5ca-43f6-b587-e58d0c1f77d3-c000.snappy.parquet',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of parquet_folder_pure: ', get_size_format(get_directory_size(parquet_folder_pure)))\n",
    "os.listdir(parquet_folder_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet was able to compress the data down to about 10% of the orginal size. It splitted the data up in 8 different data files. So every file is containing approximatly 300'000 data rows and has a size of about 3MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing using partitions\n",
    "Parquet can also store the data in different partitions wich will create a new directory for every partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first approach, we could try to create a partition for every report that means for every distinct \"adsh\" value. However, since we have about 6300 different reports in the used csv file that would result in data files less than 5kb each. Such small files are very inefficient for parquet, so we do soemthing else.<br>\n",
    "Since we read the \"ddate\" column as a proper date-format we can create partitions based on the year and month. In order to do that we need to add two columns for year and month to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_num = df_test_num.withColumn(\"year\", year(\"ddate\")).withColumn(\"month\", month(\"ddate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder_by_month = parquet_folder+\"month\"\n",
    "df_test_num.write.partitionBy('year','month').parquet(parquet_folder_by_month, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result may be a little surprising. There are folders for years starting 1978 up to 2028. The 'ddate' column is defined as \"The end date for the data value, rounded to the nearest month end\". A lot of values that appear in a report may not be from reported period. For instance, often results from the last couple of years are also included in a yearly report. Or expected returns for the following couple of years appear in the report.<br>\n",
    "The total size of on disk has also increased significantly. It is still small compared to the originial CSV file, but around 25% to 30% bigger compared to the size that was needed when the data were stored without defining partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of parquet_folder_month:  26.72MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['._SUCCESS.crc',\n",
       " 'year=1978',\n",
       " 'year=1982',\n",
       " 'year=1987',\n",
       " 'year=1989',\n",
       " 'year=1990',\n",
       " 'year=1993',\n",
       " 'year=1994',\n",
       " 'year=1995',\n",
       " 'year=1996',\n",
       " 'year=1997',\n",
       " 'year=1998',\n",
       " 'year=1999',\n",
       " 'year=2000',\n",
       " 'year=2001',\n",
       " 'year=2002',\n",
       " 'year=2003',\n",
       " 'year=2004',\n",
       " 'year=2005',\n",
       " 'year=2006',\n",
       " 'year=2007',\n",
       " 'year=2008',\n",
       " 'year=2009',\n",
       " 'year=2010',\n",
       " 'year=2011',\n",
       " 'year=2012',\n",
       " 'year=2013',\n",
       " 'year=2014',\n",
       " 'year=2015',\n",
       " 'year=2016',\n",
       " 'year=2017',\n",
       " 'year=2018',\n",
       " 'year=2019',\n",
       " 'year=2020',\n",
       " 'year=2021',\n",
       " 'year=2022',\n",
       " 'year=2023',\n",
       " 'year=2025',\n",
       " 'year=2027',\n",
       " 'year=2028',\n",
       " 'year=__HIVE_DEFAULT_PARTITION__',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of parquet_folder_month: ', get_size_format(get_directory_size(parquet_folder_by_month)))\n",
    "os.listdir(parquet_folder_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading parquet\n",
    "reading parquet is even simpler as reading a cvs since parquet contains metainformation about the file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows : 2325267\n",
      "root\n",
      " |-- adsh: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- coreg: integer (nullable = true)\n",
      " |-- ddate: date (nullable = true)\n",
      " |-- qtrs: integer (nullable = true)\n",
      " |-- uom: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_num = spark.read.parquet(parquet_folder_pure)\n",
    "print('number of rows :', df_test_num.count())\n",
    "df_test_num.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
