{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp merge_to_single_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_03_Merge_To_Single_Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this notebook is to merge all quarterly-CSV files together into one Dataframe and store it as Parquet. Moreover, a schema with the correkt datatypes is defined, so that Parquet stores the types appropriately.<br>\n",
    "In addition, the tickersymbol is also merged into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_cas_bgd_fs2020_sa.core import * # initialze spark\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-paths:  ['2019q3', '2019q4']\n",
      "All-paths:  ['2009q1', '2009q2', '2009q3', '2009q4', '2010q1', '2010q2', '2010q3', '2010q4', '2011q1', '2011q2', '2011q3', '2011q4', '2012q1', '2012q2', '2012q3', '2012q4', '2013q1', '2013q2', '2013q3', '2013q4', '2014q1', '2014q2', '2014q3', '2014q4', '2015q1', '2015q2', '2015q3', '2015q4', '2016q1', '2016q2', '2016q3', '2016q4', '2017q1', '2017q2', '2017q3', '2017q4', '2018q1', '2018q2', '2018q3', '2018q4', '2019q1', '2019q2', '2019q3', '2019q4', '2020q1', '2020q2']\n"
     ]
    }
   ],
   "source": [
    "# basic definitions\n",
    "\n",
    "# our test folder just contains the content of two zip files\n",
    "tst_csv_folders = \"./tmp/joined/\"\n",
    "tst_csv_path = Path(tst_csv_folders)\n",
    "tst_csv_path_list = [x.name for x in tst_csv_path.iterdir() if x.is_dir()]\n",
    "print(\"Test-paths: \" , tst_csv_path_list)\n",
    "\n",
    "tst_parquet_folder = \"./tmp/parquet/\"\n",
    "\n",
    "# The \"all\"-folder contains the csv files from all of the zipfiles \n",
    "all_csv_folders = \"D:/data/zip_joined/\"\n",
    "all_csv_path = Path(all_csv_folders)\n",
    "all_csv_path_list = [x.name for x in all_csv_path.iterdir() if x.is_dir()]\n",
    "print(\"All-paths: \", all_csv_path_list)\n",
    "\n",
    "all_parquet_folder = \"D:/data/parquet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27c4ab09648>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schme for reading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, DoubleType, BooleanType\n",
    "\n",
    "schema = StructType([  # num.txt  \\\n",
    "                StructField(\"adsh\", \t StringType(), True), \\\n",
    "                StructField(\"tag\", \t \t StringType(), True), \\\n",
    "                StructField(\"version\", \t StringType(), True), \\\n",
    "                StructField(\"coreg\", \t IntegerType(), True), \\\n",
    "                StructField(\"ddate\", \t DateType(), True), # date \\ \n",
    "                StructField(\"qtrs\", \t StringType(), True), \\\n",
    "                StructField(\"uom\", \t \t StringType(), True), \\\n",
    "                StructField(\"value\", \t DoubleType(), True), \\\n",
    "                StructField(\"footnote\",  StringType(), True), \\\n",
    "                      # sub.txt \\ \n",
    "                StructField(\"cik\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"name\", \t StringType(), True), \\\n",
    "                StructField(\"sic\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"countryba\", StringType(), True), \\\n",
    "                StructField(\"stprba\", \t StringType(), True), \\\n",
    "                StructField(\"cityba\", \t StringType(), True), \\\n",
    "                StructField(\"zipba\", \t StringType(), True), \\\n",
    "                StructField(\"bas1\", \t StringType(), True), \\\n",
    "                StructField(\"bas2\", \t StringType(), True), \\\n",
    "                StructField(\"baph\", \t StringType(), True), \\\n",
    "                StructField(\"countryma\", StringType(), True), \\\n",
    "                StructField(\"stprma\", \t StringType(), True), \\\n",
    "                StructField(\"cityma\", \t StringType(), True), \\\n",
    "                StructField(\"zipma\", \t StringType(), True), \\\n",
    "                StructField(\"mas1\", \t StringType(), True), \\\n",
    "                StructField(\"mas2\", \t StringType(), True), \\\n",
    "                StructField(\"countryinc\",StringType(), True), \\\n",
    "                StructField(\"stprinc\", \t StringType(), True), \\\n",
    "                StructField(\"ein\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"former\", \t StringType(), True), \\\n",
    "                StructField(\"changed\", \t StringType(), True), \\\n",
    "                StructField(\"afs\", \t \t StringType(), True), \\\n",
    "                StructField(\"wksi\", \t IntegerType(), True), \\\n",
    "                StructField(\"fye\", \t     StringType(), True), \\\n",
    "                StructField(\"form\", \t StringType(), True), \\\n",
    "                StructField(\"period\", \t DateType(), True),  # date \\\n",
    "                StructField(\"fy\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"fp\", \t \t StringType(), True), \\\n",
    "                StructField(\"filed\", \t DateType(), True), # date \\\n",
    "                StructField(\"accepted\",  StringType(), True), # datetime \\\n",
    "                StructField(\"prevrpt\", \t IntegerType(), True), \\\n",
    "                StructField(\"detail\", \t IntegerType(), True), \\\n",
    "                StructField(\"instance\",  StringType(), True), \\\n",
    "                StructField(\"nciks\", \t IntegerType(), True), \\\n",
    "                StructField(\"aciks\", \t StringType(), True), \\\n",
    "                      # pre.txt \\\n",
    "                StructField(\"report\", \t IntegerType(), True), \\\n",
    "                StructField(\"line\", \t IntegerType(), True), \\\n",
    "                StructField(\"stmt\", \t StringType(), True), \\\n",
    "                StructField(\"inpth\", \t IntegerType(), True), \\\n",
    "                StructField(\"rfile\", \t StringType(), True), \\\n",
    "                StructField(\"plabel\", \t StringType(), True), \\\n",
    "                StructField(\"negating\",  StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all csv files into one DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_239_639\n",
      "duration:  7.223994970321655\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_tst = spark.read.csv(tst_csv_folders + \"*\", header=True, dateFormat=\"yyyyMMdd\", schema=schema)\n",
    "print(\"{:_}\".format(df_tst.count())) # print number of lines in the test dataset\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and counting just the two CSV-folders is really fast, but we have to be aware that they are stored on a SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tst.show(1) # if we need to check that reading the schema was possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109_392_813\n",
      "duration:  332.6548318862915\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_all = spark.read.csv(all_csv_folders + \"*\", header=True, dateFormat=\"yyyyMMdd\", schema=schema)\n",
    "print(\"{:_}\".format(df_all.count())) # print number of lines in the whole dataset\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time, reading and counting all CSV-folders, takes about 7 minutes. But they are also read from \"normal disk\" and not a SSD. This is also clearly visible when checking the Windows Task Manager: The disk was at 100%. <br>\n",
    "It took only 45 seconds the second time and when I checked the Windows Task Manager, the CPU was at 100% and the disk was at 0%. So it looks as if the system cached the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.show(1) # if we need to check that reading the schema was possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all the contained column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote, cik, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks, report, line, stmt, inpth, rfile, plabel, negating, "
     ]
    }
   ],
   "source": [
    "_ = [print(x, end=\", \") for x in df_all.columns] # print the name of the columns for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge TickerSymbol to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the further analysis, it could make sense to know the TickerSymbol and the Exchange where the stock is traded. This information is available in a CSV located at http://rankandfiled.com/static/export/cik_ticker.csv. We simply load it into a dataframe and use the join method to join it with the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the cik-ticker dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_cik_ticker = spark.read.csv(\"./data/cik_ticker.csv\", sep=\"|\", header=True)[['CIK','Ticker','Name','Exchange']]\n",
    "# renaming the column\n",
    "df_cik_ticker = df_cik_ticker.withColumnRenamed('Name', \"name_cik_tic\") \\\n",
    "                                .withColumnRenamed('Ticker', \"ticker\") \\\n",
    "                                .withColumnRenamed('Exchange', \"exchange\") \\\n",
    "                                .withColumn(\"cik\", col(\"CIK\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+--------+\n",
      "|    cik|ticker|        name_cik_tic|exchange|\n",
      "+-------+------+--------------------+--------+\n",
      "|1090872|     A|Agilent Technolog...|    NYSE|\n",
      "|   4281|    AA|           Alcoa Inc|    NYSE|\n",
      "|1332552| AAACU|Asia Automotive A...|    null|\n",
      "|1287145|  AABB|  Asia Broadband Inc|     OTC|\n",
      "|1024015|  AABC|Access Anytime Ba...|    null|\n",
      "+-------+------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cik_ticker.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available exchanges and count of traded stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges_df = df_cik_ticker.select([\"cik\",\"exchange\"]).distinct().toPandas() # we convert to pandas in order to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0      count\n",
      "exchange        \n",
      "BATS           4\n",
      "NASDAQ      2669\n",
      "NYSE        2880\n",
      "NYSE ARCA    115\n",
      "NYSE MKT     561\n",
      "OTC         2345\n",
      "OTCBB        330\n"
     ]
    }
   ],
   "source": [
    "ct = pd.crosstab(index=exchanges_df['exchange'], columns='count')\n",
    "print(ct) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the test dataframe with the cik_ticker dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now join the test dataset together with the ticker information. We expect the same number of lines in the dataset as we had above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_239_639\n"
     ]
    }
   ],
   "source": [
    "df_tst_join = df_tst.join(df_cik_ticker, [\"cik\"], \"left\")\n",
    "print(\"{:_}\".format(df_tst_join.count())) # merge and display the number of lines in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------+--------------------+--------+\n",
      "|                adsh| cik|ticker|        name_cik_tic|exchange|\n",
      "+--------------------+----+------+--------------------+--------+\n",
      "|0000002178-19-000107|2178|    AE|Adams Resources &...|NYSE MKT|\n",
      "|0000002178-19-000107|2178|    AE|Adams Resources &...|NYSE MKT|\n",
      "+--------------------+----+------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tst_join[['adsh','cik','ticker','name_cik_tic','exchange']].show(2) # show a few columns to make sure the join worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the complete dataframe with the cik_ticker dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only create the definition of the joined dataframe, we don't execute it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the all dataset with the ticker information\n",
    "df_all_join = df_all.join(df_cik_ticker, [\"cik\"], \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing as Parquet with default partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct datytype is defined for each column and the tickersymbols have been added to the dataset. Now it is time to store the whole dataset as a new Parquet file.<br>\n",
    "We will simply use Parquet's default partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Parquet file with default partitions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  39.32404708862305\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(tst_parquet_folder,  ignore_errors=True) # make sure the target folder is empty\n",
    "\n",
    "start = time.time()\n",
    "df_tst_join.write.parquet(tst_parquet_folder)\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Parquet file with default partitions on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  758.595376253128\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(all_parquet_folder,  ignore_errors=True) # make sure the target folder is empty\n",
    "\n",
    "start = time.time()\n",
    "df_all_join.write.parquet(all_parquet_folder)\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about 13 minutes to store all the data in parquet.<br>\n",
    "Let us compare the file sizes of the compressed zip folder and the parquet folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed zip:  4.81GB\n",
      "parquet       :  5.05GB\n"
     ]
    }
   ],
   "source": [
    "print('compressed zip: ', get_size_format(get_directory_size(all_csv_folders)))\n",
    "print('parquet       : ', get_size_format(get_directory_size(all_parquet_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is about the same size, especially if we consider that the Parquet version also contains the ticker information. Moreover, Parquet contains more metainformation which will help to faster access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
